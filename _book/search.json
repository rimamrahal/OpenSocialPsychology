[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Open Social Psychology",
    "section": "",
    "text": "Preface\nSocial psychology is built on a strong set of classical research paradigms and findings, featured in many of the textbooks, syllabi, online courses and teacRima-hing guides that aspiring psychologists study with and established psychologists use as teaching resources. However, the common body of knowledge that social psychology relies on is undergoing change. Modern research methods and changing attitudes towards permissible research practices bring about social psychological research that looks different today than it used. This book is dedicated to tracing some of these changes, and to offering a version of record of the changing perceptions and interpretations of classic social psychology in the light of it’s contemporary counterpart. As such, this study book is a snapshot of how we see social psychology today.\nBecause it tends to be difficult to keep teaching and study materials up to date with emerging trends and debates, we see this study book as an addition to traditional educational resources in social psychology. It is published as an Open Educational Resource to aid the accessibility of this knowledge for all, and to be adapted to teachers’ and learners’ needs as they dive into what social psychology has to offer.",
    "crumbs": [
      "[Preface]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "howthisbook.html",
    "href": "howthisbook.html",
    "title": "How this Book Came to Be",
    "section": "",
    "text": "written by Flávio Azevedo and Rima-Maria Rahal\nSocial psychology is devoted to studying how individuals behave, think and feel within their social contexts. The field is therefore, by its very nature, set up for collaborative work. Leveraging the social context in which knowledge is generated is built in to the assumptions and interests that social psychology pursues. This fundamental attitude towards social embeddedness of knowledge is mirrored in the process by which this study book came to be.\nIt started by bringing together the work of students at Heidelberg University during the winter term of 2023. In the scope of classwork, they engaged with classical findings of social psychology, and discussed recent attempts to reengage with these classics. These works are the basis of the current book.\nResearchers working on (areas related to) social psychology then revised these chapters. Through engaging the communities at the Big Team Science Conference 2024 (BTScon), the 2025 annual meeting of the Society for the Improvement of Psychological Science (SIPS) and the Framework for Open and Reproducible Research Training (FORRT), we found collaborators willing to contribute their knowledge and expertise to turning chapter drafts into an approachable and fact checked resource.\n \nThe creation of this book was also supported by the German Reproducibility Network (GRN).\nBy co-creating educational content with diverse participants rather than relying solely on traditional authority figures, the process of writing this book explicitly built in diverse perspectives and lived experiences of groups who may otherwise not have access to such contribution opportunities. This process promoted cross-cultural scholarly exchange on replication and reproducibility in social psychology, and made this educational resource more inclusive by reflecting diverse perspectives.\nIn sum, this volume offers diverse perspectives on a shared target topic: Changing perceptions of classical social psychological research.",
    "crumbs": [
      "[How this Book Came to Be]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "howto.html",
    "href": "howto.html",
    "title": "How to Use this Book",
    "section": "",
    "text": "written by Melissa Engelbart and Rima-Maria Rahal\nThis book contains several types of resources: narrative text, definitions and questions for reflection, as well as references.\nIn fifteen chapters, we provide narrative summaries about classical research in social psychology and its modern follow-up. Often, this means we include new attempts to show the same finding (replication attempts) or meta-analytical work that brings together a lot of evidence from different sources regarding a certain hypothesis. Each chapter contains an overview of the classic study, a summary of important work thereafter, as well as a discussion of the evidence, experiments or analyses conducted. We then attempt to draw conclusions about the tested hypotheses.\nBecause this volume is targeted at students, we provide definitions of key terms, preceded by #definition and displayed like this:\n\n#definition Replication\nAn attempt to find the same result as a previous study in a new data set.\n\n\n#definition Meta-Analysis\nAn analysis that brings together evidence from several individual studies or experiments to estimate an overall effect across the available evidence.\n\nWe have aimed at providing a critical but neutral perspective to the classical and modern studies of social psychology discussed in the texts of this volume. To help you develop your own perspective and a well-reflected attitude towards this work, you will find guiding questions and suggestions that might prompt you to think more deeply about what you read throughout the book. The guiding questions cover topics such as the research and publication process itself and it’s influence on research, the interpretation of data in general, as well as the experimental operationalization of theoretical questions. Moreover, to help you consider potential applications of the findings and theories discussed, these questions sometimes ask you to think of examples or consequences in real life.\nYou’ll recognize these prompts by the preceding #yourturn. Here is an example of what these questions look like:\n\n#yourturn\nDo you think you might find such questions for reflection useful?\n\nFinally, we have enabled the option to collaboratively annotate this work using hypothesis (note that this is how links are formatted in this book) in the online version. Your annotations will be visible to others, and others will be able to see yours, so that we can build a better learning experience using this book together.\nTo read up on the original research we cite in this book, such as from Vazire (2018), you can hover over or click on the references provided.\nFeel free to make use of the resources in this book as you see fit. Our hope is that they will support you in building a well-reflected opinion about the existing body of knowledge in social psychology.\n\n\n\n\nVazire, Simine. 2018. “Implications of the Credibility Revolution for Productivity, Creativity, and Progress.” Perspectives on Psychological Science 13 (4): 411–17. https://doi.org/10.1177/1745691617751884.",
    "crumbs": [
      "[How to Use this Book]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The Role of Change for Scientific Discovery\nMuch of science capitalizes on change. It is the engine that drives progress and the expansion of knowledge (see Kuhn 1962; Popper 1959). Embracing change means taking established theories and challenging them to explore new directions. Changing perspectives, questioning the status quo, refining existing concepts, and adapting to new evidence provide the stuff that makes breakthroughs or new insights. In essence, change in science represents taking steps forward, toward greater insight and reality checks for the challenges we face. In other words, to push the boundaries of what we know, we must make change.\nIn the past decade, Open Science has made change, by transforming research practices to promote transparency, reproducibility, and collaboration in scientific endeavors. By fostering a culture of openness and collaboration, Open Science has brought about a paradigm shift in research methodologies, paving the way for more robust and reliable scientific discoveries (Munafò et al. 2017; Vazire, Schiavone, and Bottesini 2022). It is certainly no small feat to fundamentally reform how research is done, and yet we have seen significant change towards Open practices (Kidwell et al. 2016; Chambers 2019; Christensen et al. 2020).",
    "crumbs": [
      "[Introduction]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "intro.html#the-role-of-change-for-scientific-discovery",
    "href": "intro.html#the-role-of-change-for-scientific-discovery",
    "title": "Introduction",
    "section": "",
    "text": "#yourturn\nWhat instance of change regarding science have you recently heard about? Consider reports of breakthroughs you might have seen in the news or stories you saw on social media.\n\n\n\n#definition Open Science\nAn overhead term for a number of practices to make research more transparent, such as making the data a research is project is based on available to others.",
    "crumbs": [
      "[Introduction]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "intro.html#challenges-of-making-change",
    "href": "intro.html#challenges-of-making-change",
    "title": "Introduction",
    "section": "Challenges of Making Change",
    "text": "Challenges of Making Change\nChange can be a challenge because it disrupts established norms, habits, and power structures. This often means that individuals and groups might be hesitant to embrace change. Open Science as a reform to refocus on good research practice had to work with this difficulty of making change, where new methods, theories, or technologies often encounter skepticism and opposition from the scientific community. Open Science promotes transparency, data sharing, and collaborative research, which can expose flaws underlying previously held beliefs or reveal alternative interpretations. This shift can create debates about long-held ideas and established practices, which are scrutinized and potentially overturned. Established researchers may be reluctant to abandon familiar paradigms, and institutions may resist reallocating resources or altering well-known processes. Sometimes, inertia of traditional practices and fear of uncertainty can slow the adoption of innovative approaches, despite their potential to advance knowledge and solve pressing problems.\n\n#yourturn\nConsider a big change you have experienced. Was it easy to adapt to this change?\n\nHowever, a questioning attitude and focus on methodological rigor and good practice also enhance the robustness and reliability of scientific conclusions by fostering an environment where continuous re-evaluation is encouraged. Thus, Open Science exemplifies how embracing change can lead to a more dynamic and resilient understanding of the world, even as it unsettles the familiar foundations of scientific consensus.\nChange often implies the potential for a changed perception of what used to be, particularly in comparison to what is now. This is also the case in the scope of changes assosciated with Open Science. In particular, what were once considered unassailable facts can become contested or uncertain as new methodologies, data, and technologies challenge established knowledge. This is where our focus lies in this book: reporting on classical studies in social psychology and the change in how they are seen now, following a wave of additional research (often with an Open Science flavor).\n\n#yourturn\n“I was today years old when I found out …” What was the last long-held belief you had to give up?\n\nIn this spirit, when reading about the changes in perspective about classics in social psychology, there are two things to embrace:\nOn the one hand, revisiting classic social psychology studies is a demonstration of the profound impact they had on the field. Were they less important and less impactful, these studies would not draw continued debate, research interest and investment of resources. Therefore, reading classic studies can give readers a sense of what matters to social psychological research, from hot topics to hot paradigms and research methods.\nOn the other hand, following the course of the academic debate about these claims, insights and phenomena allows us to hone our skills in accumulating insights and adjusting our perception of the currently held beliefs in this area of research. Put differently, tracing efforts to replicate, to conduct meta-analyses or to establish boundary conditions to the findings postulated in a certain study mostly reflects well-intentioned interest in assessing the validity of the claims of the original study, attempting to produce clarity about our collective knowledge about the phenomenon of interest. Reassessing classical studies might require change in opinions, calibration and reflection, but it can surely spark renewed trust in research and in its ability to refine and build our joint knowledge.\n\n\n\n\nChambers, Chris. 2019. “The Registered Reports Revolution Lessons in Cultural Reform.” Significance 16 (4): 23–27.\n\n\nChristensen, Garret, Zenan Wang, Elizabeth Levy Paluck, Nicholas Swanson, David Birke, Edward Miguel, and Rebecca Littman. 2020. “Open Science Practices Are on the Rise: The State of Social Science (3S) Survey.”\n\n\nKidwell, Mallory C, Ljiljana B Lazarević, Erica Baranski, Tom E Hardwicke, Sarah Piechowski, Lina-Sophia Falkenberg, Curtis Kennett, et al. 2016. “Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency.” PLoS Biology 14 (5): e1002456.\n\n\nKuhn, Thomas. 1962. “The Structure of Scientific Revolutions.” International Encyclopedia of Unified Science 2 (2).\n\n\nMunafò, Marcus R, Brian A Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, and John Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 1–9.\n\n\nPopper, Karl R. 1959. The Logic of Scientific Discovery. Hutchinson & Co.\n\n\nVazire, Simine, Sarah R Schiavone, and Julia G Bottesini. 2022. “Credibility Beyond Replicability: Improving the Four Validities in Psychological Science.” Current Directions in Psychological Science 31 (2): 162–68.",
    "crumbs": [
      "[Introduction]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Ego Depletion",
    "section": "",
    "text": "1.1 The Classic\nEgo depletion is a social psychological concept that describes the depletion of individuals’ self-regulatory resources. Baumeister et al. (1998) were the first to demonstrate ego depletion effects in four different experimental settings: After having to engage in an act of self-control (compared to a control task that does not require self-control), willpower is used up and could not be deployed as effectively in a subsequent task.\nIn Experiment 1, the focus was on the act of resisting a temptation, which requires self-control. Participants were randomly assigned to different food conditions, by which the independent variables were manipulated: Chocolate chip cookies and chocolate, radishes or no food at all (control group). Participants in the radish control condition were instructed to resist the tempting chocolates and instead eat several the radishes that were laid out next to the chocolate. In the chocolate condition, participants were asked to eat several cookies or chocolates, which were laid out next to the radishes – a task that was not supposed to require much self-control. The actual intention behind the experiment, to demonstrate ego depletion, was disguised with a cover story to make sure participants would not get suspicious. They were told the experiment was about taste perception.\nIn the no-food control condition, participants were not asked to taste any food, but worked on the rest of the experiment.\nAfter the participants had completed the willpower task resisting the temptation of the foods presented to them, they had to complete questionnaires on mood and restraint. Then they had to work on “solving” a problem-solving task, which was actually unsolvable. Here, the time spent on trying to solve the problem before giving up was the dependent variable.\nThe results showed significant differences between the three conditions, with participants in the radish condition stopping earlier than those in the chocolate or no-food condition. In conclusion, it was suggested that craving chocolate but choosing to eat radishes depleted an internal resource, leaving individuals less able to persist while trying to solve the puzzles afterwards.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Ego Depletion]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-classic",
    "href": "chapter1.html#the-classic",
    "title": "1  Ego Depletion",
    "section": "",
    "text": "#definition Ego Depletion\nA concept that describes willpower as a limited resource that can be used up (depleted).\n\n\n\n#yourturn\nWhich other tasks in your daily life require more or less willpower?\n\n\n\n\n\n#yourturn\nIf willpower can be depleted, how can it be “refilled” or built up again?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Ego Depletion]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-aftermath",
    "href": "chapter1.html#the-aftermath",
    "title": "1  Ego Depletion",
    "section": "1.2 The Aftermath",
    "text": "1.2 The Aftermath\nSince this study, several hundred follow-up studies, including several multi-lab studies that aimed to replicate the overall finding (Hagger et al. 2010; Vohs et al. 2021) and several meta-analyses (Hagger et al. 2010; Carter and McCullough 2014; Dang 2017; Blázquez, Botella, and Suero 2017) have been carried out.\n\n#definition Multi-Lab Study\nA research project in which researchers working at several different locations (laboratories) implement the same experimental design and then analyse the data together.\n\nThese studies yielded mixed results, with some concluding that it was highly unlikely that the ego depletion phenomenon does not exist (e.g., Hagger et al. 2010), while others failed to establish the effect despite relying on data from more than 2000 participants (e.g., Hagger et al. 2010). Publication bias has been argued to be high in the literature on ego depletion (Inzlicht, Gervais, and Berkman 2015), casting doubt on the effect.\n\n#definition Publication Bias\nA tendency for research in line with established theories or showing significant results to be more easily publishable than deviating research.\n\nContinued research interest on ego depletion has brought forward varying hypotheses regarding circumstances under which the effect might be demonstrable and robust. The meta-analysis on ego depletion conducted by Dang (2017) investigated only studies with sufficient initial effort exerted in the depleting, which was hypothesized to lead to the ego depletion effect. The study ensured that the depleting task required the use of self-control and excluded manipulations that were less clearly related to self-control, such as those based on social exclusion. Eight commonly used depletion tasks were assessed in the meta-analysis: attention essay, attention video, crossing out letters, emotion video, food trial, Stroop, thought suppression, and working memory.\n\n#yourturn\nCan you imagine what participants had to do in these tasks? Think about a version of each task that would drain self-control and one that would be less exhausting.\n\nThe results showed that two of these exhausting tasks, attention video and working memory, were not associated with significant changes in subsequent self-control. Emotion videos, on the other hand, appeared to be the most effective task and reduced subsequent self-control.\nThe overall analysis revealed a small to medium effect size for the ego depletion effect. Correcting for publication bias, this effect was not statistically significant when using the full sample of studies identified. However, a separate analysis for reliable depletion tasks, such as attention essay, emotion video and Stroop, showed the significant effect remained when attempting to correct for publication bias. This meta-analysis suggests that in special tasks, ego depletion might occur, but that it is difficult to generalize to other circumstances.\nHowever, even in these special tasks, there is often no direct measure of the initial depletion of willpower involved: manipulation checks on whether willpower has been used up offer only an indirect measurement (Friese et al. 2018).\n\n#yourturn\nHow could you objectiveley measure the amount of willpower available or drained?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Ego Depletion]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1  Ego Depletion",
    "section": "1.3 Conclusion",
    "text": "1.3 Conclusion\nThe literature suggests a differentiated view on the potentially finite nature of willpower is necessary (for a detailed overview, read more in Friese et al. 2018). In the context of social psychological theories, the ego depletion effect can be seen as an important example of contradictory findings in research, where publication bias may play a role. Although several hundred studies on ego depletion have been published, we cannot be sure whether ego depletion exists or not.\n\n#yourturn\nDo you think ego depletion exists?\n\nThe debate about ego depletion shows that individual findings should be reassessed in several empirical demonstrations, including replication attempts that can provide a more realistic picture of the effect or construct. In this case, the original ego depletion effect may have been initially inflated due to publication bias. Following closer examination, it is less certain whether this effect indeed exists. The example of the ego depletion literature also shows the importance of examining the evidence closely, under the microscope, in order to ensure that it meets the quality criteria that are essential for assessing cumulative evidence of the overall effect.\n\n\n\n\nBaumeister, Roy F., Ellen Bratslavsky, Mark Muraven, and Dianne M. Tice. 1998. “Ego Depletion: Is the Active Self a Limited Resource?” Journal of Personality and Social Psychology 74 (5): 1252–65. https://doi.org/10.1037/0022-3514.74.5.1252.\n\n\nBlázquez, Desirée, Juan Botella, and Manuel Suero. 2017. “The Debate on the Ego-Depletion Effect: Evidence from Meta-Analysis with the p-Uniform Method.” Frontiers in Psychology 8 (February). https://doi.org/10.3389/fpsyg.2017.00197.\n\n\nCarter, Evan C., and Michael E. McCullough. 2014. “Publication Bias and the Limited Strength Model of Self-Control: Has the Evidence for Ego Depletion Been Overestimated?” Frontiers in Psychology 5 (July). https://doi.org/10.3389/fpsyg.2014.00823.\n\n\nDang, Junhua. 2017. “An Updated Meta-Analysis of the Ego Depletion Effect.” Psychological Research 82 (4): 645–51. https://doi.org/10.1007/s00426-017-0862-x.\n\n\nFriese, Malte, David D. Loschelder, Karolin Gieseler, Julius Frankenbach, and Michael Inzlicht. 2018. “Is Ego Depletion Real? An Analysis of Arguments.” Personality and Social Psychology Review 23 (2): 107–31. https://doi.org/10.1177/1088868318762183.\n\n\nHagger, Martin S., Chantelle Wood, Chris Stiff, and Nikos L. D. Chatzisarantis. 2010. “Ego Depletion and the Strength Model of Self-Control: A Meta-Analysis.” Psychological Bulletin 136 (4): 495–525. https://doi.org/10.1037/a0019486.\n\n\nInzlicht, Michael, Will Gervais, and Elliot Berkman. 2015. “Bias-Correction Techniques Alone Cannot Determine Whether Ego Depletion Is Different from Zero: Commentary on Carter, Kofler, Forster, & Mccullough, 2015.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2659409.\n\n\nVohs, Kathleen D., Brandon J. Schmeichel, Sophie Lohmann, Quentin F. Gronau, Anna J. Finley, Sarah E. Ainsworth, Jessica L. Alquist, et al. 2021. “A Multisite Preregistered Paradigmatic Test of the Ego-Depletion Effect.” Psychological Science 32 (10): 1566–81. https://doi.org/10.1177/0956797621989733.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Ego Depletion]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  False Consensus Effect",
    "section": "",
    "text": "2.1 The Classic\nThe false consensus effect is a cognitive bias in which individuals overestimate the extent to which their own beliefs, preferences, and behaviors are shared by others.\nThis psychological phenomenon was first systematically studied by Ross, Greene, and House (1977), who demonstrated that individuals tend to perceive their own choices and opinions as more common than they actually are. For instance, people who express a preference for a particular option are likely to assume that others would make the same choice, even when evidence suggests otherwise. This bias occurs because individuals use their own perspective as a reference point, leading to distorted judgments about the preferences, opinions and behaviors of others.\nIn Study 1 of the original research by Ross, Greene, and House (1977), participants were presented with one of four short stories, each describing a fictional scenario with a behavioral choice to be made. After reading the assigned story, participants were asked to estimate the percentage of their peers who would choose one behavioral option over the other within the context of the story.\nFollowing these percentage estimates, participants completed a questionnaire. First, they were required to indicate which behavioral option they personally would have chosen in the scenario. Next, they rated themselves on a personality scale. As part of the assessment, participants also evaluated the typical personality characteristics of someone their age and gender who would choose either behavioral option presented in the story.\nThe results revealed a consistent pattern: participants who chose a particular behavioral option tended to believe that “people in general” would likely make the same choice. Conversely, participants who rejected an option perceived that behavior as less likely for others. Across all four stories, participants’ own choices strongly predicted their estimates of how the general population would behave.\nAdditionally, significant differences emerged in personality evaluations based on participants’ own choices. For three of the four stories, participants rated the typical personality traits of those choosing their preferred behavioral option as less extreme than those who selected the alternative. These effects were statistically significant in three stories, while one story showed a weaker significance, and the fourth story showed no significant results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[False Consensus Effect]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter2.html#the-classic",
    "href": "chapter2.html#the-classic",
    "title": "2  False Consensus Effect",
    "section": "",
    "text": "#definition Bias\nA systematic distortion of perception or judgment.\n\n\n\n#definition False Consensus Effect\nA cognitive bias where individuals overestimate the extent to which others share their beliefs, preferences, and behaviors.\n\n\n\n#yourturn\nCan you think of a time when you assumed others thought or behaved the same way you did and it turned out to not be the case?\n\n\n\n\n\n#yourturn\nAre there certain methodological choices that could enhance or reduce the magnitude of the false consensus effect? These could include, but are not limited to, the number of choices to choose from, the social setting, the controversiality of the choices and the order of choices. Do they increase or reduce the magnitude of the false consensus effect?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[False Consensus Effect]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter2.html#the-aftermath",
    "href": "chapter2.html#the-aftermath",
    "title": "2  False Consensus Effect",
    "section": "2.2 The Aftermath",
    "text": "2.2 The Aftermath\nA meta-analysis by Mullen et al. (1985) examined 23 studies and a total of 115 hypotheses related to the false consensus effect. The analysis demonstrated that tests for the false consensus effect were highly significant and produced a moderate effect size. Importantly, it identified specific methodological factors that influenced the magnitude of the effect. For instance, the number of behavioral decisions participants were asked to make, as well as the order in which decisions and consensus estimates were presented, significantly impacted the observed false consensus effect.\n\n#definition Effect Size\nA quantitative measure of the magnitude of a phenomenon, used to assess the practical significance of research findings.\n\nThese findings suggested that subsequent studies should limit the number of behavioral decisions participants are required to make and prioritize consensus assessments before behavioral decisions, as those methodological peculiarities might maximize the observed extent of the false consensus effect in experimental settings.\nThe self-presentation explanation posits that individuals strategically align their behavior with perceived social norms. According to this theory, the false consensus effect should be more pronounced when individuals make their behavioral decision before estimating the consensus. Only in this sequence do participants have the chance to adjust the social norm (i.e., other people’s behavior) to their own behavior. However, the meta-analysis by Mullen et al. (1985) found no statistical evidence supporting this prediction, suggesting that the false consensus effect does not vary as the self-presentation explanation would anticipate.\n\n#yourturn\nWhich other mechanisms could explain the False Consensus Effect? How would you test those mechanisms?\n\nmullen_false_1985 outlined several theoretical explanations for the false consensus effect. One explanation, attributive projection, suggests that individuals rely on cognitive biases to justify their belief that their own behavioral choices are rational and appropriate responses to the environment. Another perspective suggests that the false consensus effect can protect a person’s self-esteem. It may help people feel better about themselves when they face failure or receive negative feedback about their personal characteristics. A third explanation focuses on social environments, noting that people tend to associate with others who share similar backgrounds, values, and interests. Using false consensus makes us associate with the others who are (often falsely) perceived to be similar, thus fulfilling the need for a sense of relatedness. This selective association reinforces the perception that their choices are widely shared. Finally, cognitive availability provides a more mechanistic account, proposing that the behaviors individuals have chosen—or would choose—are more easily recalled or imagined than alternative actions when theorising about the behavior of others, a phenomenon linked to the availability heuristic.\n\n#definition Availability Heuristic\nA mental shortcut where people estimate the likelihood of an event based on how easily examples come to mind, which can lead to overestimating rare but memorable occurrences.\n\nOverall, the false consensus effect is often attributed to a psychological desire to see one’s thoughts and actions as appropriate, normal, and correct. Together, these cognitive and motivational factors help explain why individuals consistently overestimate the prevalence of their own opinions and behaviors, a phenomenon observed across numerous studies.\nRecent research has refined our understanding of the false consensus effect, particularly by situating it in contemporary social and digital contexts. In a series of studies, Bunker and Varnum (2021) found that greater social media use was reliably associated with stronger false consensus effects across domains such as political attitudes, personality traits, and social motives. However, the size of these effects was consistently smaller than laypeople anticipated, suggesting a public overestimation of social media’s distorting power. Luzsa and Mayr (2021) experimentally demonstrated that exposure to attitudinally congruent news feeds, especially those with high agreement and visible endorsement cues like “likes”, leads individuals to overestimate public support for their own views. Interestingly, this inference was moderated by participants’ interest in the topic, with highly engaged individuals showing more skepticism toward consensus cues.\nBuilding on the political implications of false consensus, Steiner, Landwehr, and Harms (2025) found that individuals who overestimate how many others share their political preferences are more likely to express populist attitudes and to distrust political elites. Similarly, Weinschenk, Panagopoulos, and Linden (2021) showed that individuals’ views about democratic norms, such as the peaceful transfer of power, were strongly linked to their perceptions of what others believe—indicating a false consensus bias, particularly among conservatives. Finally, Furnas and LaPira (2024) extended the scope of the false consensus effect to unelected political elites (e.g., lobbyists and journalists) demonstrating that this group’s perceptions of public opinion systematically reflected their own views, suggesting egocentrism rather than ideological bias as the driving force.\nTogether, these studies demonstrate that the false consensus effect is a robust phenomenon with wide-ranging relevance from digital communication to political judgment and that it is shaped not only by cognitive mechanisms but also by the structural, technological, and ideological environments in which opinions are formed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[False Consensus Effect]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "chapter2.html#conclusion",
    "href": "chapter2.html#conclusion",
    "title": "2  False Consensus Effect",
    "section": "2.3 Conclusion",
    "text": "2.3 Conclusion\nThe body of research on the false consensus effect highlights its robustness as a psychological phenomenon while also revealing important complexities in how it comes about. Early experimental studies, such as those by Ross, Greene, and House (1977), demonstrated that individuals consistently overestimate the degree to which others share their beliefs and behaviors. Follow-up meta-analyses, like that of Mullen et al. (1985), confirmed the effect’s significance and explored the methodological and contextual factors that influence its magnitude.\nIn the broader context of social psychology, the false consensus effect provides valuable insights into how cognitive biases and motivational factors shape human perception. Explanations for the effect, ranging from attributive projection and ego defense to mechanisms like cognitive availability, underline the interplay between how individuals view themselves and how they perceive the social world around them.\nHowever, as with many constructs in psychology, it is crucial to approach findings on the false consensus effect with careful scrutiny. Methodological variations can significantly impact the observed magnitude of the effect, and further research is needed to disentangle its underlying mechanisms. The enduring study of the false consensus effect is an example of the importance of revisiting and refining theoretical constructs to build a more comprehensive understanding of human cognition and behavior.\n\n\n\n\nBunker, Cameron J., and Michael E. W. Varnum. 2021. “How Strong Is the Association Between Social Media Use and False Consensus?” Computers in Human Behavior 125 (December): 106947. https://doi.org/10.1016/j.chb.2021.106947.\n\n\nFurnas, Alexander C., and Timothy M. LaPira. 2024. “The People Think What I Think: False Consensus and Unelected Elite Misperception of Public Opinion.” American Journal of Political Science 68 (3): 958–71. https://doi.org/10.1111/ajps.12833.\n\n\nLuzsa, Robert, and Susanne Mayr. 2021. “False Consensus in the Echo Chamber: Exposure to Favorably Biased Social Media News Feeds Leads to Increased Perception of Public Support for Own Opinions.” Cyberpsychology: Journal of Psychosocial Research on Cyberspace 15 (1). https://doi.org/10.5817/CP2021-1-3.\n\n\nMullen, Brian, Jennifer L Atkins, Debbie S Champion, Cecelia Edwards, Dana Hardy, John E Story, and Mary Vanderklok. 1985. “The False Consensus Effect: A Meta-Analysis of 115 Hypothesis Tests.” Journal of Experimental Social Psychology 21 (3): 262–83. https://doi.org/10.1016/0022-1031(85)90020-4.\n\n\nRoss, Lee, David Greene, and Pamela House. 1977. “The ‘False Consensus Effect’: An Egocentric Bias in Social Perception and Attribution Processes.” Journal of Experimental Social Psychology 13 (3): 279–301. https://doi.org/10.1016/0022-1031(77)90049-X.\n\n\nSteiner, Nils D., Claudia Landwehr, and Philipp Harms. 2025. “False Consensus Beliefs and Populist Attitudes.” Political Psychology 00: 1–22. https://doi.org/10.1111/pops.70026.\n\n\nWeinschenk, Aaron C., Costas Panagopoulos, and Sander van der Linden. 2021. “Democratic Norms, Social Projection, and False Consensus in the 2020 U.S. Presidential Election.” Journal of Political Marketing 20 (3-4): 255–68. https://doi.org/10.1080/15377857.2021.1939568.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[False Consensus Effect]{style=\"color: #004b23\"}</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Take-Aways\nAdd here",
    "crumbs": [
      "[Summary]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "summary.html#thanks",
    "href": "summary.html#thanks",
    "title": "Summary",
    "section": "Thanks",
    "text": "Thanks\nThis book was made possible by the many helping hands and critical thoughts of the student authors involved in writing the individual chapters. In addition, Melissa Engelbarth’s support with selecting and translating the chapters to include was invaluable.",
    "crumbs": [
      "[Summary]{style=\"color: #004b23\"}"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baumeister, Roy F., Ellen Bratslavsky, Mark Muraven, and Dianne M. Tice.\n1998. “Ego Depletion: Is the Active Self a Limited\nResource?” Journal of Personality and Social Psychology\n74 (5): 1252–65. https://doi.org/10.1037/0022-3514.74.5.1252.\n\n\nBlázquez, Desirée, Juan Botella, and Manuel Suero. 2017. “The\nDebate on the Ego-Depletion Effect: Evidence from Meta-Analysis with the\np-Uniform Method.” Frontiers in Psychology 8 (February).\nhttps://doi.org/10.3389/fpsyg.2017.00197.\n\n\nBunker, Cameron J., and Michael E. W. Varnum. 2021. “How Strong Is\nthe Association Between Social Media Use and False Consensus?”\nComputers in Human Behavior 125 (December): 106947. https://doi.org/10.1016/j.chb.2021.106947.\n\n\nCarter, Evan C., and Michael E. McCullough. 2014. “Publication\nBias and the Limited Strength Model of Self-Control: Has the Evidence\nfor Ego Depletion Been Overestimated?” Frontiers in\nPsychology 5 (July). https://doi.org/10.3389/fpsyg.2014.00823.\n\n\nChambers, Chris. 2019. “The Registered Reports Revolution Lessons\nin Cultural Reform.” Significance 16 (4): 23–27.\n\n\nChristensen, Garret, Zenan Wang, Elizabeth Levy Paluck, Nicholas\nSwanson, David Birke, Edward Miguel, and Rebecca Littman. 2020.\n“Open Science Practices Are on the Rise: The State of Social\nScience (3S) Survey.”\n\n\nDang, Junhua. 2017. “An Updated Meta-Analysis of the Ego Depletion\nEffect.” Psychological Research 82 (4): 645–51. https://doi.org/10.1007/s00426-017-0862-x.\n\n\nFriese, Malte, David D. Loschelder, Karolin Gieseler, Julius\nFrankenbach, and Michael Inzlicht. 2018. “Is Ego Depletion Real?\nAn Analysis of Arguments.” Personality and Social Psychology\nReview 23 (2): 107–31. https://doi.org/10.1177/1088868318762183.\n\n\nFurnas, Alexander C., and Timothy M. LaPira. 2024. “The People\nThink What I Think: False Consensus and\nUnelected Elite Misperception of Public Opinion.” American\nJournal of Political Science 68 (3): 958–71. https://doi.org/10.1111/ajps.12833.\n\n\nHagger, Martin S., Chantelle Wood, Chris Stiff, and Nikos L. D.\nChatzisarantis. 2010. “Ego Depletion and the Strength Model of\nSelf-Control: A Meta-Analysis.” Psychological Bulletin\n136 (4): 495–525. https://doi.org/10.1037/a0019486.\n\n\nInzlicht, Michael, Will Gervais, and Elliot Berkman. 2015.\n“Bias-Correction Techniques Alone Cannot Determine Whether Ego\nDepletion Is Different from Zero: Commentary on Carter, Kofler, Forster,\n& Mccullough, 2015.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2659409.\n\n\nKidwell, Mallory C, Ljiljana B Lazarević, Erica Baranski, Tom E\nHardwicke, Sarah Piechowski, Lina-Sophia Falkenberg, Curtis Kennett, et\nal. 2016. “Badges to Acknowledge Open Practices: A Simple,\nLow-Cost, Effective Method for Increasing Transparency.” PLoS\nBiology 14 (5): e1002456.\n\n\nKuhn, Thomas. 1962. “The Structure of Scientific\nRevolutions.” International Encyclopedia of Unified\nScience 2 (2).\n\n\nLuzsa, Robert, and Susanne Mayr. 2021. “False Consensus in the\nEcho Chamber: Exposure to Favorably Biased Social Media\nNews Feeds Leads to Increased Perception of Public Support for Own\nOpinions.” Cyberpsychology: Journal of Psychosocial Research\non Cyberspace 15 (1). https://doi.org/10.5817/CP2021-1-3.\n\n\nMullen, Brian, Jennifer L Atkins, Debbie S Champion, Cecelia Edwards,\nDana Hardy, John E Story, and Mary Vanderklok. 1985. “The False\nConsensus Effect: A Meta-Analysis of 115 Hypothesis\nTests.” Journal of Experimental Social Psychology 21\n(3): 262–83. https://doi.org/10.1016/0022-1031(85)90020-4.\n\n\nMunafò, Marcus R, Brian A Nosek, Dorothy VM Bishop, Katherine S Button,\nChristopher D Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan\nWagenmakers, Jennifer J Ware, and John Ioannidis. 2017. “A\nManifesto for Reproducible Science.” Nature Human\nBehaviour 1 (1): 1–9.\n\n\nPopper, Karl R. 1959. The Logic of Scientific Discovery.\nHutchinson & Co.\n\n\nRoss, Lee, David Greene, and Pamela House. 1977. “The ‘False\nConsensus Effect’: An Egocentric Bias in Social\nPerception and Attribution Processes.” Journal of\nExperimental Social Psychology 13 (3): 279–301. https://doi.org/10.1016/0022-1031(77)90049-X.\n\n\nSteiner, Nils D., Claudia Landwehr, and Philipp Harms. 2025.\n“False Consensus Beliefs and Populist Attitudes.”\nPolitical Psychology 00: 1–22. https://doi.org/10.1111/pops.70026.\n\n\nVazire, Simine. 2018. “Implications of the Credibility Revolution\nfor Productivity, Creativity, and Progress.” Perspectives on\nPsychological Science 13 (4): 411–17. https://doi.org/10.1177/1745691617751884.\n\n\nVazire, Simine, Sarah R Schiavone, and Julia G Bottesini. 2022.\n“Credibility Beyond Replicability: Improving the Four Validities\nin Psychological Science.” Current Directions in\nPsychological Science 31 (2): 162–68.\n\n\nVohs, Kathleen D., Brandon J. Schmeichel, Sophie Lohmann, Quentin F.\nGronau, Anna J. Finley, Sarah E. Ainsworth, Jessica L. Alquist, et al.\n2021. “A Multisite Preregistered Paradigmatic Test of the\nEgo-Depletion Effect.” Psychological Science 32 (10):\n1566–81. https://doi.org/10.1177/0956797621989733.\n\n\nWeinschenk, Aaron C., Costas Panagopoulos, and Sander van der Linden.\n2021. “Democratic Norms, Social\nProjection, and False Consensus\nin the 2020 U.S. Presidential\nElection.” Journal of Political Marketing\n20 (3-4): 255–68. https://doi.org/10.1080/15377857.2021.1939568.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "In this glossary, you can find the terms defined in this book in alphabetical order. Headings are clickable links that show you how the context of the term in one of the chapters of this book.\n\nAvailability Heuristic\nA mental shortcut where people estimate the likelihood of an event based on how easily examples come to mind, which can lead to overestimating rare but memorable occurrences.\n\n\nBias\nA systematic distortion of perception or judgment.\n\n\nEffect Size\nA quantitative measure of the magnitude of a phenomenon, used to assess the practical significance of research findings.\n\n\nEgo Depletion\nA concept that describes willpower as a limited resource that can be used up (depleted).\n\n\nFalse Consensus Effect\nA cognitive bias where individuals overestimate the extent to which others share their beliefs, preferences, and behaviors.\n\n\nMulti-Lab Study\nA research project in which researchers working at several different locations (laboratories) implement the same experimental design and then analyse the data together.",
    "crumbs": [
      "[Glossary]{style=\"color: #004b23\"}"
    ]
  }
]